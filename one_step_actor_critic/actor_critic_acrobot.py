# -*- coding: utf-8 -*-
"""Actor Critic Corrected - Acrobot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1khXWG9ewg7T2zFMstj0ZM65snTJoV7rk
"""

import gym
import numpy as np
from tqdm import tqdm_notebook
import torch
import torch.nn as nn
import torch.nn.functional as func
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"

class PolicyNetwork(nn.Module):
  def __init__(self, num_observations, num_actions, hidden_layer_size):
    super(PolicyNetwork, self).__init__()
    self.input_layer = nn.Linear(num_observations, hidden_layer_size)
    self.output_layer = nn.Linear(hidden_layer_size, num_actions)
    # self.input_layer.weight.data = torch.zeros_like(self.input_layer.weight.data)
    # self.output_layer.weight.data = torch.zeros_like(self.output_layer.weight.data)
  def forward(self, s):
    s = self.input_layer(s)
    s = func.relu(s)
    actions = self.output_layer(s)
    action_probs = func.softmax(actions, dim=1)
    return action_probs

class StateValueNetwork(nn.Module):
  def __init__(self, num_observations, hidden_layer_size):
    super(StateValueNetwork, self).__init__()
    self.input_layer = nn.Linear(num_observations, hidden_layer_size)
    self.output_layer = nn.Linear(hidden_layer_size, 1)
    # self.input_layer.weight.data = torch.zeros_like(self.input_layer.weight.data)
    # self.output_layer.weight.data = torch.zeros_like(self.output_layer.weight.data)

  def forward(self, s):
    s = self.input_layer(s)
    s = func.relu(s)
    return self.output_layer(s)

def init_networks(env, hidden_layer_size):
  num_observations = env.observation_space.shape[0]
  num_actions = env.action_space.n
  policy_network = PolicyNetwork(num_observations, num_actions, hidden_layer_size).to(device)
  policy_optim = optim.SGD(policy_network.parameters(), lr=0.0008)
  state_value_network = StateValueNetwork(num_observations, hidden_layer_size).to(device)
  state_value_optim = optim.SGD(state_value_network.parameters(), lr=0.003)
  return policy_network, policy_optim, state_value_network, state_value_optim

def train(policy_optim, state_value_optim, pol_loss, state_value_loss):
  policy_optim.zero_grad()
  pol_loss.backward(retain_graph=True)
  policy_optim.step()
  state_value_optim.zero_grad()
  state_value_loss.backward()
  state_value_optim.step()

def get_action_probs(s, policy_network):
  s = torch.from_numpy(s).float().unsqueeze(0).to(device)
  action_probs = policy_network(s)
  s = s.detach()
  return action_probs

def get_loss(reward, gamma, state_value, next_state_value, log_prob, I):
  adv = reward + gamma * next_state_value.item() - state_value.item()
  pol_loss = -log_prob * adv
  pol_loss *= I
  #print(pol_loss)
  state_value_loss = func.mse_loss(reward + gamma * next_state_value, state_value)
  state_value_loss *= I
  #print(state_value_loss)
  return state_value_loss, pol_loss

def actor_critic(env, eps, gamma, hidden_layer_size):
  policy_network, policy_optim, state_value_network, state_value_optim = init_networks(env, hidden_layer_size)
  max_steps = 10000
  final_rewards = []
  for episode in tqdm_notebook(range(eps)):
    s = env.reset()
    curr_r = 0
    I = 1
    for step in range(max_steps):
      action_probs = get_action_probs(s, policy_network)
      categ = Categorical(action_probs)
      sampled_a = categ.sample()
      a = sampled_a.item()
      log_prob = categ.log_prob(sampled_a)
      next_state, reward, done, _ = env.step(a)
      curr_r += reward
      s_tens = torch.from_numpy(s).float().unsqueeze(0).to(device)
      next_s_tens = torch.from_numpy(next_state).float().unsqueeze(0).to(device)
      state_value = state_value_network(s_tens)
      next_state_value = state_value_network(next_s_tens)
      if done:
          next_state_value = torch.tensor([0]).float().unsqueeze(0).to(device)
      state_value_loss, pol_loss = get_loss(reward, gamma, state_value, next_state_value, log_prob, I)
      train(policy_optim, state_value_optim, pol_loss, state_value_loss)
      if done:
          break
      s = next_state
      I *= gamma
    #print(curr_r)
    final_rewards.append(curr_r)
  env.close()
  return final_rewards, policy_network

env = gym.make('Acrobot-v1')
eps = 1000
gamma = 0.99
hidden_layer_size = 128
final_rewards, policy_network = actor_critic(env, eps, gamma, hidden_layer_size)

plt.clf()
graph_scores = [-1*x for x in final_rewards]
plt.plot(graph_scores)
plt.ylabel('Number of steps')
plt.xlabel('episodes')
plt.title('Acrobot - One Step Actor Critic')
plt.show()

#Testing
from gym.wrappers import RecordVideo
done = False
s = env.reset()
env = gym.wrappers.RecordVideo(env, '/content/drive/MyDrive/RL/Actor_Critic/', name_prefix="Acrobot")
done = False
r = 0
while not done:
  action_probs = get_action_probs(s, policy_network)
  categ = Categorical(action_probs)
  sampled_a = categ.sample()
  a = sampled_a.item()
  log_prob = categ.log_prob(sampled_a)
  next_state, reward, done, info = env.step(a)
  r += reward
  s = next_state
print(r)

