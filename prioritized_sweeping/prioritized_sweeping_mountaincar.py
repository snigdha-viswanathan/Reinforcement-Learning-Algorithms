# -*- coding: utf-8 -*-
"""Prioritized_sweeping-MountainCar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cpYPlv7McuFZ71Akl_7pXvjE_vQCsjeX
"""

from ctypes import sizeof
import gym
import numpy as np
from collections import defaultdict
import random

# Create Acrobot environment
env = gym.make('MountainCar-v1')
env._max_episode_steps = 2000
# Constants
gamma = 0.9  # Discount factor
alpha = 0.1 # Learning rate
n = 10  # Number of updates per iteration
epsilon = 0.1
# Tile coding parameters
num_tilings = 4
num_tiles = 4
num_actions = env.action_space.n
theta = 0.00001
# Define state space limits for normalization
state_space_low = env.observation_space.low
state_space_high = env.observation_space.high

# Compute scale and offset for normalization
scale = num_tiles / (state_space_high - state_space_low)
offset = -state_space_low * scale


# Function to perform tile encoding
def tile_encode(state):
    state_scaled = (state + offset) * scale
    tilings = []
    for i in range(num_tilings):
        tiling = []
        for j in range(len(state)):
            tiling.append(int(state_scaled[j] + i * (num_tiles / num_tilings)))
        tilings.append(tuple(tiling))
    return tilings

# Initialize Q-values, Model, and priority queue
Q_values = defaultdict(lambda: np.random.uniform(0,1,num_actions))
model = defaultdict(lambda: [0, [0]*num_actions])  # Storing rewards and next states for each action
priority_queue = defaultdict(float)

# Function to update the model with observed transition
def update_model(s, a, r, s_):
    state_tiles = tile_encode(s)
    next_state_tiles = tile_encode(s_)
    model[tuple(state_tiles + [a])] = (r, next_state_tiles)

# Prioritized sweeping algorithm using tile encoding
def prioritized_sweeping():
    predecessors = {}
    steps = []
    for _ in range(500):
        state = env.reset()
        t = 0
        while True:
            t = t+1
            state_tiles = tile_encode(state)
            # print(state_tiles)
            # if(np.random.rand()<epsilon):
            #   action = np.random.choice(len(Q_values[tuple(state_tiles)]))
            # else:
            #   action = np.argmax(Q_values[tuple(state_tiles)])
            action = np.argmax(Q_values[tuple(state_tiles)])
            next_state, reward, done, _ = env.step(action)
            update_model(state, action, reward, next_state)
            q_max = np.max(Q_values[tuple(tile_encode(next_state))])
            priority = np.abs(reward + gamma * q_max - Q_values[tuple(state_tiles)][action])
            if priority > theta:
                priority_queue[tuple(state_tiles + [action])] = priority
            if tuple(state_tiles) not in predecessors.keys():
            for _ in range(n):
                if len(priority_queue) == 0:
                    break
                print(priority_queue)
                max_priority_item = max(priority_queue, key=priority_queue.get)
                s, a = max_priority_item[:-1], max_priority_item[-1]
                r, s_ = model[s + tuple([a])]
                q_max = np.max(Q_values[tuple(s_)])
                Q_values[tuple(s)][a] += alpha * (r + gamma * q_max - Q_values[tuple(s)][a])
                del priority_queue[max_priority_item]
                for state_action_pair in model.keys():
                    state, action = state_action_pair[:-1], state_action_pair[-1]
                    predicted_next_state = model[state_action_pair][1]
                    if(tuple(s) == tuple(predicted_next_state)):
                      predicted_reward = model[state_action_pair][0]
                      q_max = np.max(Q_values[tuple(predicted_next_state)])
                      TD_error = np.abs(predicted_reward + gamma * q_max - Q_values[tuple(state)][action])
                      if TD_error > theta:
                          priority_queue[tuple(state + tuple([action]))] = TD_error

            if done:
                steps.append(t)
                # print(t)
                break

            state = next_state
    return Q_values, steps

# Run the prioritized sweeping algorithm with tile encoding
q_values, steps = prioritized_sweeping()

## After learning Q-values (assuming Q_values is the learned Q-values dictionary)
import matplotlib.pyplot as plt

# Using the learned Q-values to act in the environment
#epsilon = 0.1  # Epsilon value for epsilon-greedy strategy
num_episodes = 1  # Number of episodes to run
env._max_episode_steps = 2000
env = gym.wrappers.RecordVideo(env, '/content/drive/MyDrive/videos', name_prefix="acrobot")
for _ in range(num_episodes):
    state = env.reset()
    done = False
    step_count = 0
    # Create a figure to display the frames
    #plt.figure()
    while not done:
        state_tiles = tile_encode(tuple(state))
        action = np.argmax(q_values[tuple(state_tiles)])
        next_state, reward, done, _ = env.step(action)
        state = next_state
        step_count = step_count + 1
        # Render the environment if needed
        #frame = env.render(mode='rgb_array')
        # plt.imshow(frame)
        # plt.axis('off')
        # plt.title(f"Step: {env._elapsed_steps}")  # Accessing step count; specific to this environment
        # plt.pause(0.01)
      # Create a figure to display the frames
    #plt.close()
    print(step_count)
# Remember to close the environment when done
env.close()

import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
import numpy as np

sns.set()
plt.clf()
plt.plot(steps)
plt.ylabel('Number of steps')
plt.xlabel('episodes')
plt.title('MountainCar - Priority Sweeping')

tick_interval = 3000
plt.yticks(np.arange(min(steps), max(steps) + 1, tick_interval))

plt.show()

