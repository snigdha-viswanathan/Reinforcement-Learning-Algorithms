# -*- coding: utf-8 -*-
"""Prioritized sweeping-687gridworld.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1prgnTx9soH4D4yFQOG1EFudEswysMDP0
"""

#from ctypes import sizeof
import gym
import numpy as np
from decimal import Decimal
import numpy as np
from collections import defaultdict
from queue import PriorityQueue
# Create Acrobot environment
#env = gym.make('FrozenLake-v1')
#env._max_episode_steps = 20000
# Constants
gamma = 1.0  # Discount factor
alpha = 0.1 # Learning rate
n = 50 # Number of updates per iteration
epsilon = 0.1
num_actions = 4
theta = 0.0001
rows = 5
cols = 5
#num_states = env.observation_space.n
# Initialize Q-values, Model, and priority queue
# Q_values = np.zeros((num_states, num_actions))
#Q_values = np.zeros((num_states, num_actions))
#Q_values = defaultdict(lambda: np.zeros(num_actions))
# model = {}
# #model = defaultdict(lambda: [0, [0]*num_actions])
# priority_queue = PriorityQueue()# Storing rewards and next states for each action
# states_visited = {}
#priority_queue = defaultdict(float)
#print(priority_queue.get())

import random

all_states = [(0, 0), (0, 1), (0, 2), (0, 3), (0,4), (1, 0), (1, 1), (1, 2), (1, 3), (1,4),(2, 0), (2, 1), (2, 3), (2,4),(3, 0), (3, 1), (3, 3), (3,4),(4, 0), (4, 1), (4, 2), (4, 3)]  # Define your states here
print(len(all_states))
values = [1/22 for _ in range(22)]
def generate_initial_state():
    # Generate a random initial state uniformly from all possible states
    initial_state = random.choices(all_states,weights = values)[0]
    return initial_state

def transition_probability(current_state, action, new_state):
    r, c = current_state
    r_prime, c_prime = new_state
    base_prob = 0

    if (r==2 and c==2) or (r==3 and c==2):
          return Decimal('0.0')
    if (r_prime == 2 and c_prime == 2) or (r_prime == 3 and c_prime == 2):
      return Decimal('0.0')
    # Calculate transition probabilities based on action and dynamics
    if action == 0:  # AttemptUp
        if r_prime == r and c_prime == c:
            # Stay in the same state due to temporary break (10% probability)
            base_prob += Decimal('0.1')

        if r==0:
          if current_state == new_state:
            base_prob += Decimal('0.8')
        else:
          if (r==4 and c==2) and  current_state == new_state :
            base_prob += Decimal('0.8')
          elif r_prime == r - 1 and c_prime == c:
            base_prob += Decimal('0.8')

        if c==4:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if ((r==2 and c==1) or (r==3 and c==1)) :
            if current_state == new_state:
              base_prob += Decimal('0.05')
          elif r_prime == r and c_prime == c + 1:
            base_prob += Decimal('0.05')

        if c==0:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if ((r==2 and c==3) or (r==3 and c==3)) :
            if current_state == new_state:
              base_prob += Decimal('0.05')
          elif r_prime == r and c_prime == c - 1:
            base_prob += Decimal('0.05')
        if r==4 and c==4:
          return Decimal('0.0')

    if action == 1:  # Attemptdown
        if r_prime == r and c_prime == c:
            # Stay in the same state due to temporary break (10% probability)
            base_prob += Decimal('0.1')

        if r==4:
          if current_state == new_state:
            base_prob += Decimal('0.8')
        else:
          if (r==1 and c==2) and  current_state == new_state :
            base_prob += Decimal('0.8')
          elif r_prime == r + 1 and c_prime == c:
            base_prob += Decimal('0.8')

        if c==0:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if ((r==2 and c==3) or (r==3 and c==3)) :
            if current_state == new_state:
              base_prob += Decimal('0.05')
          elif r_prime == r and c_prime == c - 1:
            base_prob += Decimal('0.05')

        if c==4:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if ((r==2 and c==1) or (r==3 and c==1)) :
            if current_state == new_state:
              base_prob += Decimal('0.05')
          elif r_prime == r and c_prime == c + 1:
            base_prob += Decimal('0.05')
        if r==4 and c==4:
          return Decimal('0.0')
    if action == 2:  # Attemptleft
        if r_prime == r and c_prime == c:
            # Stay in the same state due to temporary break (10% probability)
            base_prob += Decimal('0.1')

        if c==0:
          if current_state == new_state:
            base_prob += Decimal('0.8')
        else:
          if ((r==2 and c==3) or (r==3 and c==3)):
            if current_state == new_state :
              base_prob += Decimal('0.8')
          elif r_prime == r and c_prime == c-1:
            base_prob += Decimal('0.8')

        if r==0:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if (r==4 and c==2)  and  current_state == new_state :
            base_prob += Decimal('0.05')
          elif r_prime == r-1 and c_prime == c:
            base_prob += Decimal('0.05')

        if r==4:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if (r==1 and c==2)  and  current_state == new_state :
            base_prob += Decimal('0.05')
          elif r_prime == r+1 and c_prime == c:
            base_prob += Decimal('0.05')
        if r==4 and c==4:
          return Decimal('0.0')

    if action == 3:  # Attemptright
        if r_prime == r and c_prime == c:
            # Stay in the same state due to temporary break (10% probability)
            base_prob += Decimal('0.1')

        if c==4:
          if current_state == new_state:
            base_prob += Decimal('0.8')
        else:
          if ((r==2 and c==1) or (r==3 and c==1)) :
            if current_state == new_state:
              base_prob += Decimal('0.8')
          elif r_prime == r and c_prime == c+1:
            base_prob += Decimal('0.8')

        if r==4:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if (r==1 and c==2)  and  current_state == new_state :
            base_prob += Decimal('0.05')
          elif r_prime == r+1 and c_prime == c:
            base_prob += Decimal('0.05')

        if r==0:
          if current_state == new_state:
            base_prob += Decimal('0.05')
        else:
          if (r==4 and c==2)  and  current_state == new_state :
            base_prob += Decimal('0.05')
          elif r_prime == r-1 and c_prime == c:
            base_prob += Decimal('0.05')
        if r==4 and c==4:
          return Decimal('0.0')
    return base_prob

def get_next_state(state, action):
  r = state[0]
  c = state[1]
  #print(state)
  p1 = transition_probability(state, action, (r+1,c))
  p2 = transition_probability(state, action, (r-1,c))
  p3 = transition_probability(state, action, (r,c+1))
  p4 = transition_probability(state, action, (r,c-1))
  p5 = transition_probability(state, action, (r,c))
  probabilities = [p1, p2, p3, p4, p5]
  #print(probabilities)
  # total_prob = sum(probabilities)
  # probabilities = [prob / total_prob for prob in probabilities]
  chosen_index = np.random.choice(len(probabilities), p = probabilities)
  if(chosen_index == 0):
    return (r+1,c)
  if(chosen_index == 1):
    return (r-1,c)
  if(chosen_index == 2):
    return (r,c+1)
  if(chosen_index == 3):
    return (r,c-1)
  if(chosen_index == 4):
    return (r,c)

def reward_function(next_state):
  r_prime, c_prime = next_state
  if(r_prime == 4 and c_prime == 4):
    return 10
  elif(r_prime == 4 and c_prime == 2):
    return -10
  else:
    return 0

# Initialize Q-function arbitrarily
def initialize_q(num_actions=4):
  q = {}
#np.random.rand()
# Randomly initialize values for each key
  for i in range(rows):
    for j in range(cols):
        # Assuming 'actions' is 4
        q[(i, j)] = [np.random.rand() for _ in range(4)]  # Initialize actions as random values
        #q[(i, j)] = [0.0 for _ in range(4)]
  q[(2,2)] = [0.0 for _ in range(4)]
  q[(3,2)] = [0.0 for _ in range(4)]
  q[(4,4)] = [0.0 for _ in range(4)]
  return q

priority_queue = PriorityQueue()
states_visited = {}  # nxtState -> list[(curState, Action)...]
model = {}
Q_values = initialize_q()
states_set = set()
def epsilon_greedy_action(Q, state, epsilon=epsilon):
    if random.uniform(0,1) < epsilon:
        # return np.random.choice(len(Q[state]))  # Choose random action
        return random.randint(0, len(Q[state])-1)
    else:
        # return np.argmax(Q[state])  # Choose action with max Q-value
        return Q[state].index(max(Q[state]))

def epsilon_decay(initial_epsilon, episode, decay_rate):
    min_epsilon = 0.1
    # epsilon = min_epsilon + (initial_epsilon - min_epsilon) / (1 + decay_rate * episode)
    epsilon = initial_epsilon / (1 + decay_rate * episode)
    return epsilon

#epsilon = 0.3
#steps = [0 for i in range(50)]
steps = []
# for run in range(1):
steps_ep=0
priority_queue = PriorityQueue()
states_visited = {}  # nxtState -> list[(curState, Action)...]
model = {}
Q_values = initialize_q()

#print(Q_values)
for _ in range(50):  # Perform a fixednumber of iterations
      state = generate_initial_state()
      #state = (0,0)
      #print(state)
      #print(state)  # Initial state
      # Loop through each state
      t = 0
      while state != (4,4):
          if(np.random.uniform(0,1)<epsilon):
            action = np.random.choice(len(Q_values[state]))
          else:
            action = np.argmax(Q_values[state])
          #action = np.argmax(Q_values[state])
          #action = epsilon_greedy_action(Q_values, state)
          #print(action)
          next_state = get_next_state(state,action)
          #print(next_state)
          reward = reward_function(next_state)
          #next_state, reward, done, _ = env.step(action)
          #print(done,reward,"-",next_state)
          # Update the model with the observed transition
          model[(state, action)] = (reward, next_state)
          # print(model)
          # Calculate the temporal difference (TD) error
          #np.argmax(Q[state])
          q_max = np.max(Q_values[next_state])
          #print(q_max)
          TD_error = np.abs(reward + (gamma * q_max )- Q_values[state][action])
          #print(TD_error)
          # If the TD_error is non-zero, insert the state-action pair into the priority queue
          if TD_error > theta:
            if state not in states_set:
              priority_queue.put((-TD_error, (state,action)))
              states_set.add(state)
          # print("st:",states_set)
          #print(priority_queue)
          # Perform prioritized updates
          if next_state not in states_visited.keys():
            states_visited[next_state] = [(state, action)]
          else:
            states_visited[next_state].append((state, action))
          state = next_state
          steps_ep += reward
          t+=1
          for _ in range(n):
              if priority_queue.empty():
                break
              (s, a) = priority_queue.get()[1]
              #print(s,a)
            # Update Q-values for the highest priority state-action pair
              r, s_ = model[(s, a)]
              #r, s_ = model[tuple(s + [a])]
              #print(s)
              # Update Q-values for the highest priority state-action pair
              q_max = np.max(Q_values[s_])
              Q_values[s][a] += alpha * (r + gamma * q_max - Q_values[s][a])

              # Remove the state-action pair from the priority queue
              #del priority_queue[max_priority_item]
              # Loop for all S', A' predicted to lead to S
              if s in states_visited.keys():
                s_a_list = states_visited[s]
                for (prev_s, prev_a) in s_a_list:
                  prev_r = model[(prev_s,prev_a)][0]
                  prev_q_max = np.max(Q_values[s])
                  prev_TD_error = np.abs(reward + (gamma * q_max )- Q_values[prev_s][prev_a])
                  if prev_TD_error > theta:
                    if prev_s not in states_set:
                      priority_queue.put((-prev_TD_error, (prev_s,prev_s)))
                      states_set.add(prev_s)
              # for (ss, aa), (rr, _) in model.items():
              #     # print("New tate:",ss)
              #     #print(ss)
              #     # print("Old state:",s)
              #     if ss == s:
              #     #state_n, action_n = state_action_pair[:-1], state_action_pair[-1]
              #     #print(state_n)

              #     #predicted_next_state = model[state_action_pair][1]
              #     #print(predicted_next_state)
              #     #if(tuple(state_n) == tuple(s)):
              #       # predicted_reward = model[state_action_pair][0]
              #       #print(predicted_next_state)
              #       q_max = np.max(Q_values[s])
              #       TD_error = np.abs(rr + gamma * q_max - Q_values[ss][aa])
              #       #print(TD_error)
              #       if TD_error > theta:
              #           #print("Hi")
              #           priority_queue.put((-TD_error, (ss,aa)))
              #       # else:
              #       #/   print("Bye")
          # if done:
          #     #steps.append(t)
                #print(t)
          #     break
      #epsilon = epsilon_decay(epsilon, episode, 0.001)
      #steps[episode] = steps[episode] + steps_ep
      steps.append(steps_ep)   #state = next_state
      print(steps_ep)#print(Q_values)

fin_steps_epi = [x/1 for x in steps]
print(len(fin_steps_epi))

import matplotlib.pyplot as plt

x_values = fin_steps_epi
y_values = [x for x in range(550)]

fig, ax = plt.subplots(figsize=(10, 5))

# Plotting the graph
plt.plot(y_values, x_values)

# Adding labels to the axes
plt.xlabel('Episodes')
plt.ylabel('Cumulative rewards')

# Adding a title to the graph
plt.title('Gridworld')

# plt.xticks(range(round(min(steps_count)), round(max(steps_count))+1, 1000))

# Display the graph
plt.show()
# sns.set()
# plt.clf()
# plt.plot(fin_steps_epi)
# plt.ylabel('Number of steps')
# plt.xlabel('Rewards')
# plt.title('Froxzen lake - Priority Sweeping')

# tick_interval = 3000
# plt.yticks(np.arange(min(steps), max(steps) + 1, tick_interval))
# reg = LinearRegression().fit(np.arange(len(steps)).reshape(-1, 1), np.array(steps).reshape(-1, 1))
# y_pred = reg.predict(np.arange(len(steps)).reshape(-1, 1))
# plt.plot(y_pred)
# plt.show()

