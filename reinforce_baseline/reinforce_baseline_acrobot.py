# -*- coding: utf-8 -*-
"""Reinforce Baseline Corrected - Acrobot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oL4BwODSAzoBpZrf93wf8S1OPK3DOKOi
"""

import gym
import numpy as np
from tqdm import tqdm_notebook
import torch
import torch.nn as nn
import torch.nn.functional as func
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"

class PolicyNetwork(nn.Module):
  def __init__(self, num_observations, num_actions, hidden_layer_size):
    super(PolicyNetwork, self).__init__()
    self.input_layer = nn.Linear(num_observations, hidden_layer_size)
    self.output_layer = nn.Linear(hidden_layer_size, num_actions)
  def forward(self, s):
    s = self.input_layer(s)
    s = func.relu(s)
    actions = self.output_layer(s)
    action_probs = func.softmax(actions, dim=1)
    return action_probs

class StateValueNetwork(nn.Module):
  def __init__(self, num_observations, hidden_layer_size):
    super(StateValueNetwork, self).__init__()
    self.input_layer = nn.Linear(num_observations, hidden_layer_size)
    self.output_layer = nn.Linear(hidden_layer_size, 1)
    # self.input_layer.weight.data = torch.zeros_like(self.input_layer.weight.data)
    # self.output_layer.weight.data = torch.zeros_like(self.output_layer.weight.data)

  def forward(self, s):
    s = self.input_layer(s)
    s = func.relu(s)
    return self.output_layer(s)

def init_networks(env, hidden_layer_size):
  num_observations = env.observation_space.shape[0]
  num_actions = env.action_space.n
  policy_network = PolicyNetwork(num_observations, num_actions, hidden_layer_size).to(device)
  policy_optim = optim.Adam(policy_network.parameters(), lr=0.0008)
  state_value_network = StateValueNetwork(num_observations, hidden_layer_size).to(device)
  state_value_optim = optim.Adam(state_value_network.parameters(), lr=0.008)
  return policy_network, policy_optim, state_value_network, state_value_optim

def train_policy(delta_values, log_probs, policy_optim):
  pol_loss = []
  for delta, log_prob in zip(delta_values, log_probs):
    pol_loss.append(-delta * log_prob)
  # print("policy-loss: ", pol_loss)
  policy_optim.zero_grad()
  sum(pol_loss).backward()
  policy_optim.step()

def train_state_value(state_values, state_value_optim, G):
  state_value_loss = func.mse_loss(state_values, G)
  # print("val-loss: ", state_value_loss)
  state_value_optim.zero_grad()
  state_value_loss.backward()
  state_value_optim.step()

def get_action_probs(s, policy_network):
  s = torch.from_numpy(s).float().unsqueeze(0).to(device)
  action_probs = policy_network(s)
  s = s.detach()
  return action_probs

def get_state_values(states, state_value_network):
  state_values = []
  for s in states:
    s = torch.from_numpy(s).float().unsqueeze(0).to(device)
    state_values.append(state_value_network(s))
  state_values = torch.stack(state_values).squeeze()
  return state_values

def get_discounted_rewards(gamma, rewards):
  temp_reward = 0
  G = []
  for reward in reversed(rewards):
      temp_reward = reward + gamma * temp_reward
      G.insert(0, temp_reward)
  G = torch.tensor(G).to(device)
  G = (G - G.mean())/G.std()
  return G

def reinforce_with_baseline(env, eps, gamma, hidden_layer_size):
  policy_network, policy_optim, state_value_network, state_value_optim = init_networks(env, hidden_layer_size)
  max_steps = 10000
  # env._max_episode_steps = max_steps
  final_rewards = []
  for episode in tqdm_notebook(range(eps)):
    s = env.reset()
    states = []
    actions = []
    rewards = []
    log_probs = []
    curr_r = 0
    for step in range(max_steps):
      action_probs = get_action_probs(s, policy_network)
      categ = Categorical(action_probs)
      sampled_a = categ.sample()
      a = sampled_a.item()
      log_prob = categ.log_prob(sampled_a)
      next_state, reward, done, _ = env.step(a)
      states.append(s)
      actions.append(a)
      rewards.append(reward)
      log_probs.append(log_prob)
      curr_r += reward
      if done:
          # print("ep: ",episode, "-", step)
          break
      s = next_state
    final_rewards.append(curr_r)
    G = get_discounted_rewards(gamma, rewards)
    state_values = get_state_values(states, state_value_network)
    train_state_value(state_values, state_value_optim, G)
    delta_values = [g_val - state_val for g_val, state_val in zip(G, state_values)]
    delta_values = torch.tensor(delta_values).to(device)
    # log_probs = torch.tensor(log_probs).to(device).requires_grad_()
    train_policy(delta_values, log_probs, policy_optim)
    #print(curr_r)
  env.close()
  return final_rewards, policy_network

env = gym.make('Acrobot-v1')
eps = 500
gamma = 0.99
hidden_layer_size = 128
final_rewards, policy_network = reinforce_with_baseline(env, eps, gamma, hidden_layer_size)

plt.clf()
graph_scores = [-1*x for x in final_rewards]
plt.plot(graph_scores)
plt.ylabel('Number of steps')
plt.xlabel('episodes')
plt.title('Acrobot - Reinforce')
plt.show()

from gym.wrappers import RecordVideo
done = False
state = env.reset()
final_scores = []

#Testing
env = gym.wrappers.RecordVideo(env, '/content/drive/MyDrive/RL/', name_prefix="Acrobot")
s = env.reset()
done = False
r = 0
while not done:
  action_probs = get_action_probs(s, policy_network)
  categ = Categorical(action_probs)
  sampled_a = categ.sample()
  a = sampled_a.item()
  log_prob = categ.log_prob(sampled_a)
  next_state, reward, done, info = env.step(a)
  r += reward
  s = next_state
print(r)